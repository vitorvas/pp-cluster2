\documentclass[twoside,a4paper,12pt,english]{inac19}

%INAC2019 SETUP: SET PAGE SIZE AND SET FOR USING graphicx PACKAGE.
\usepackage{graphicx}
\usepackage{babel,varioref,epsfig} %,rotating}
\usepackage{amssymb}
\usepackage[font=bf,center]{caption}
\usepackage{subfigure}

\usepackage{textcomp}             % Para usar marca registrada

%\title{MANAGING AND USING A PROFESSIONAL CLUSTER IN NUCLEAR ENGINEERING APPLICATIONS}
\title{THE LTHN COMPUTER CLUSTER}

%INAC2019 SETUP: SPECIFY AUTHOR NAMES, AFFILIATION, ADDRESS AND E-MAIL.

\author
  {{\bfseries{\normalsize Vitor Vasconcelos A. Silva$^{1}$, Graiciany P. Barros$^{1}$}} \\
  {\bfseries{\normalsize {Andr\'e A. C. dos Santos$^{1,2}$, Daniel A. Magalh\~aes Campolina$^{1}$}}} \\ \\
    $^{1}$Centro de Desenvolvimento da Tecnologia Nuclear (CDTN/CNEN)\\
    Av. Presidente Ant\^onio Carlos, 6627 \\
    31270-901 Belo Horizonte - MG\\
    vitors@cdtn.br, graiciany.barros@cdtn.br, aacs@cdtn.br, campolina@cdtn.br\\ \\
    $^{2}$Departamento de Engenharia Nuclear - UFMG \\
    Universidade Federal de Minas Gerais\\
    Av. Presidente Ant\^onio Carlos, 6627 \\
    31270-901 Belo Horizonte - MG\\
    }
%\author{
%  \bf{Vitor Vasconcelos, Andr\'e dos Santos,}\\
%  \bf{Daniel Campolina and Graiciany Barros}\\ \\
%  CDTN - Centro de Desenvolvimento da Tecnologia Nuclear\\
%  Av. Ant\^onio Carlos 6627 - Campus UFMG\\
%  31270-901 - Belo Horizonte, MG\\
%  \{vitors, aacs, campolina, graiciany.barros\}@cdtn.br}



\begin{document}

%INAC2019 SETUP: PRINT TITLE
\maketitle

% Acrescentado para facilitar a redação da NI associada
%\tableofcontents
%\tableofcontents


%INAC2019 SETUP: SETUP HEADS FOR PAGES
\pagestyle{myheadings}
\thispagestyle{empty}
\markboth{}{}


%INAC2019 SETUP: SET FIRST PAGE WITH NO PAGE NUMBER
\thispagestyle{empty}

%--------------------------------------------------------------------------------------
\begin{abstract_full_paper}
Computers have become a fundamental tool at almost every human activity.
In science, they're ubiquitous and used so widely that are often neglected
as a complex tool. A professional computer cluster is a good example of
this situation. Many high-end computers in expensive hardware configuration
put together using state-of-the-art software tools usually overlooked by
users as a 'set of computers'. In this paper we present such scientific tool
in a particular context: a high-end cluster system installed, managed and operated
by the very same scientific team which employs it on research on the field of Nuclear Engineering.
This work describes the main features of the LTHN/CDTN (Thermal-hydraulics and Neutronics
Laboratory/Nuclear Technology Development Center), considering software solutions
adopted, the impact of it on research currently carried on at the same laboratory
and the future possible applications of the system. A special attention is given to
the distributed file-system, a fundamental feature to allow high-performance in a system
with many physical disk storage units and to the monitoring system. The latter being
the 'eyes' of the users into the system's load and performance. All solutions adopted for the
cluster are based on open or free software and almost the totality of the research
software used is also open or free. Definitely the path towards less expensive
research, a theme of special interest for scientists and researches from developing countries.
\end{abstract_full_paper}

%--------------------------------------------------------------------------------------
\section{INTRODUCTION}\label{int}

This paper presents the current situation of the LTHN/CDTN (Thermal-hydraulics and Neutronics
Laboratory/Nuclear Technology Development Center) computer cluster. This cluster is the main 
computational tool of the research group and it is mainly used on Neutronics and Thermal-hydraulics 
numerical simulations. In this work it is briefly described, considering hardware and software, 
with special focus on the software solutions which allows its operation on a 24/7 basis.

The current work does not focus on hardware characteristics nor network configuration since 
a solid description of these features is provided in a former work of the team responsible 
for the cluster operation\cite{cluster17}.

%------------------------------------------------------------------------------
\subsection{Context}

It is probably unnecessary to mention the importance of computers to all the work carried on 
in the field of Science and Technology. Put shortly: the magnitude of calculations and simulations 
achieved using computers - in this case, a special set of them - are simply impossible without 
them and this fact makes computers integral part of scientific development. An elegant outlook 
on this matter is given by Dongarra\cite{Dongarra2017}.

%------------------------------------------------------------------------------
\section{OBJECTIVE}

The objective of this work is to present the current status of the cluster equipament 
at the Laboratory of Thermal-hydraulics and Neutronics of CDTN (\textbf{LTHN}), describing the decisions 
taken during the installation process, the solutions applied to common (and uncommon) problems 
derived from running a professional cluster system. Together with technical decisions, the 
performance of the system is asserted by comparison to the older system used by the laboratory 
team. 


%------------------------------------------------------------------------------
\section{METHODS AND PROCEDURES}

*************************************************************\\
Main information to be detailed: the cluster uses hostbased authentication, GPU used via 
vnc, \texttt{data} directory at each user account pointing to gluster, how many gluster 
volumes (why two, how they are configured - distributed) and what each one stores, 
the NAS configuration (related to the cluster).

Must update the old figures accordingly. Pehaps add two more figures showing NAS 
and on the internal structure of gluster/NAS.

List the applications running (if possible with some benchmarks) and installed. 
Future work, if something is still missing.

************************************************************\\

%------------------------------------------------------------------------------
\subsection{Hardware}

The \textit{slaves} are Dell workstations Precision R7910 equipped
with Dual Intel{\textregistered} Xeon{\textregistered} processors E5-2640 operating at $2.4$GHz with
$32$Gb of RAM each. They have two $10$GbE (Gigabit Ethernet) and also two $1$Gbit network connectors, which
allow the use of the Gigabit network exclusively for intra-cluster communication. Their disks are $360$Gb SSD
and are used to store the Operating System of each node and also part of the \textit{Gluster} distributed
file system. These equipments are also installed with Nvidia{\textregistered} GPUs (Graphic Processing Units) M4000{\textregistered} with $8$Gb CUDA\cite{CUDA} capable.

\begin{figure}[h] % t forces top and b forces bottom: can be added to h, ex. [ht]
  \centering\includegraphics[scale=0.7]{images/cluster-topologico.png}
  \caption{Cluster hardware.}
  \label{fig:cluster}
\end{figure}

The GPUs on nodes are used as calculation units by the software which can take advantage of this kind of hardware.
The use of hardware acceleration in scientific computing is not new and have been increasing consistently on the
last years\cite{accelerators}.

The \textit{master} node is a Dell Precision T7910. The main
differences between the \textit{master} node and the \textit{slaves} machines are two: the \textit{master} has
a slightly worst graphics card, a Quadro{\textregistered} M2000 with $4$Gb of memory. However, the second difference
is the ammount of RAM memory. The nodes are equipped with $32$Gb of memory whereas the \textit{master} has
$256$Gb of RAM.


Three hard-disks of $500$Gb are physically connect to the \textit{master} node and are used as part of the 
\textit{Gluster} distributed file system, offering
an extra $1.5$Tb of storage for the whole system.


%--------------------------------------------------------%

%------------------------------------------------------------------------------
\subsubsection{NAS}

XXXXXXXXXXXXXXXXXXXXx

%------------------------------------------------------------------------------
\subsection{Software Solutions}
\label{sub:ssol}


It is worth mention that among the main software solutions a simple and yet powerful package 
allows the use of embedded temperature monitoring hardware to display system's temperature. 
Although now only used on system administrator needs, it can be used to automate system's 
responses in specific events. Of course, on top of the already available operational systems 
safety measures. 

%------------------------------------------------------------------------------
\subsection{File System}

During the installation phase of the cluster system\cite{cluster17} there were two the contenders for
distributed filesystem solution. The chosen one is described below.

%------------------------------------------------------------------------------
\subsubsection{Gluster File System}

GlusterFS\cite{gluster} is an open source, distributed file system and has
a unique no-metadata server architecture. This is the chosen solution for the system due to its relatively
small number of physical disks and nodes (nine).

The system is configured to use two \textit{gluster} volumes, namely \texttt{gv-apps} and \texttt{gv-data}. Both volumes
are configured as \textit{distributed} without replication. Since the whole system has an extra NAS device, there is
no need of keeping redundant data. The \texttt{gv-data} offers $N$ Gb of storage per user and is used to run simulations or any task that needs storage during execution. After that, data must be moved to a storage space on NAS where an automatized regular backup avoids data loss.

Figure \ref{fig:cluster-gluster} shows the "software" view of \textit{gluster} by the system's operational system. The big 
gluster disk is, in reality, a set of different hard disks grouped in the mentioned volumes and controlled by the 
\textit{gluster} daemons running on each node and the control daemon running only on the master node.

\begin{figure}[h] % t forces top and b forces bottom: can be added to h, ex. [ht]
%  \centering\includegraphics[width=8.5cm,height=8.5cm]{images/esquema_cluster_edited_bw.png}
  \centering\includegraphics[scale=0.7]{images/cluster-gluster.png}
  \caption{``View'' of \textit{Gluster} distributed filesystem.}
  \label{fig:cluster-gluster}
\end{figure}

%------------------------------------------------------------------------------
\subsection{Job scheduler}
\label{ssec:slurm}

%Jobs: Tenho que explicar o Slurm\cite{slurm} e cita-lo.


\subsubsection{System use: basics}
The access to the cluster system is made as simple as possible. However, the absence of a Graphical User Interface
(GUI) slightly increases system's difficult of use, specially for those not used to command line. That said, only
a handful of commands are necessary to login, check jobs queue and submit a job. System's login screen is shown
in Figure \ref{fig:login-screen}.

\begin{figure}[h] % t forces top and b forces bottom: can be added to h, ex. [ht]
  \centering\includegraphics[scale=0.7]{images/p1login.png}
  \caption{LTHN cluster login screen.}
  \label{fig:login-screen}
\end{figure}

Of course, a basic understanding on how to use Linux shell can be of great value of productivity for users.
Those users with basic knowledge on shell scripting are the best served as they will be able to fine tuning
their simulations trought scripts. That said, a basic recommendation to all users is to carefully read and
study Chapter 1 and 2 of reference\cite{ECP}.

%------------------------------------------------------------------------------
\subsection{System monitoring tool}
\label{ssec:ganglia}

At this point is clear the importance to be able to submit jobs, have them scheduled following priorities
and having a load balacing scheme to help optimizing the clusters resources. Nonetheless, the ability to
monitor (or check) system's load on different nodes, taking account different resources - memory, disk, IO,
etc. - without the need to log in to the system is 

http: Tenho que explicar o Ganglia\cite{ganglia} e cita-lo. Ao menos citar o Apache\cite{apache}.

\begin{figure}[h] % t forces top and b forces bottom: can be added to h, ex. [ht]
%  \centering\includegraphics[width=8.5cm,height=8.5cm]{images/esquema_cluster_edited_bw.png}
  \centering\includegraphics[scale=0.55]{images/ganglia_rainbow.png}
  \caption{A color graphic showing relative node use.}
  \label{fig:ganglia-rainbow}
\end{figure}

\begin{figure}[h] % t forces top and b forces bottom: can be added to h, ex. [ht]
  \centering\includegraphics[scale=0.7]{images/ganglia_colors.png}
  \caption{Color reference screen showing nodes loads.}
  \label{fig:ganglia-colors}
\end{figure}

\begin{figure}[h] % t forces top and b forces bottom: can be added to h, ex. [ht]
  \centering\includegraphics[scale=0.5]{images/ganglia_nodes.png}
  \caption{Nodes load on the monitoring system.}
  \label{fig:ganglia-nodes}
\end{figure}

%------------------------------------------------------------------------------

\section{RESULTS AND ANALYSIS}

At the time of writing these lines, the system is up and running non-stop for 92 days. This is a
basic measurement of it robustness. A total of $907$ jobs were submitted since the installation
of the scheduler, including failing jobs,
test jobs and also working cases jobs. The bulk of most submitted jobs are thermal-hydraulics
simulations using ANSYS CFX and ANSYS Fluent. Only a part of all simulations are related to neutronics.
The reason for that is fairly simple: the workflow is completely dictated by the needs of users
and during the last 2 months of system monitoring most sheduled works were from students of mechanical
and aerospatial engineering finishing their final undergraduation works.

In order to present some quantitative results two neutronics problems were modelled. The hardware used
to run these problems is:

\begin{itemize}
\item Desktop: Intel(R) Xeon(R) CPU E5520 @ 2.27GHz, 8 CPUs, 48Gb RAM.
\item Nodes: Intel(R) Xeon(R) CPU E5-2640 v4 @ 2.40GHz, 20 CPUs, 32Gb RAM.
\end{itemize}

The number of available nodes is 8, although not used for both simulations.

One simulation was run using Serpent Nuclear code, installed in both the cluster and desktop machines at the Thermal-Hydraulics
and Neutronics Laboratory (LTHN), version 2.1.31 compiled to make use of OpenMP\cite{openMP}. This simulation involves burning
significative number of different isotopes and is considered the heavy test presented in this work. It is worth mentioning
that, due to lack of a proven thread-safe MPI implementation, the compiled version of Serpent runs only in one machine.
This translates to the maximum of one node running on its full: 20 processors.

The results of the this benchmark are presented in Table \ref{table:benchmark-serpent}.

\begin{table}[ht]
  \caption{Serpent benchmark}
  \centering
\begin{tabular}{c|c|r}
Machine & \begin{tabular}[c]{@{}c@{}}Number\\ of processors\end{tabular} & \multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}}Elapsed\\ time\end{tabular}} \\ \hline
Desktop & 8                                                              & 00:21:34                                                                   \\
Node    & 8                                                              & 00:12:46                                                                   \\
Node    & 20                                                             & 00:05:12                                                                   \\
Node    & 160                                                            & 00:00:42                                                                  
\end{tabular}
\label{table:benchmark-serpent}
\end{table}

The second benchmark uses MCNP6 and is a fairly simple simulation, consisting in simulating the response in dosis
to a set of detectors scattered in surfaces in a room with a defined radiation source. This model
was run using 8 processors (the maximum available at the desktop machine) in both desktop
and node, plus 20 processors and 160 processors in the cluster.

The results of MCNP simulations are shown in Table \ref{table:benchmark-mcnp}.

\begin{table}[ht]
  \caption{MCNP benchmark}
  \centering
\begin{tabular}{c|c|r}
Machine & \begin{tabular}[c]{@{}c@{}}Number\\ of processors\end{tabular} & \multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}}Elapsed\\ time\end{tabular}} \\ \hline
Desktop & 8                                                              & 2 days, 08:26:43                                                                   \\
Node    & 8                                                              & 1 day, 11:02:36                                                                   \\
Node    & 20                                                             & 23:19:46                                                                   
\end{tabular}
\label{table:benchmark-mcnp}
\end{table}


%------------------------------------------------------------------------------

\section{CONCLUSIONS AND PERSPECTIVES}


(O que está bom, o que está ruim na atual instalação)
(Trabalhos futuros: pra onde ir (operação), possíveis mudanças)
(Olhar no plans o que falta)

%Uma cita\c{c}\~{a}o \cite{Henderson17}.


%------------------------------------------------------------------------------



\section*{Acknowledgments}
The authors would like to thank FUJB for financing the cluster acquisition
as part of the project \textit{Desenvolvimento de novos elementos combust\'{i}veis nucleares
  e materiais e pe\c{c}as para combust\'{i}veis nucleares}, agreement FINEP 01.07.0548.00 - Process FUJB 13.867-3.
The authors also thank Dr. Jo\~{a}o Roberto Loureiro de Mattos, now retired, for the work which made possible the acquisition of the cluster.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{thebibliography}{99} %99 é o número máximo que o thebibliography permite. Numero de referencias que aparecerão.

\bibitem{cluster17} Vitor V. A. Silva, Andr\'{e} A. C. dos Santos and Renan O. Cunha, ``Professional Cluster Management
  by a small scientific team: challenges, solutions and perspectives'', \textit{XX ENFIR INAC 2017 International Nuclear Atlantic Conference }, Belo Horizonte, MG, Brazil, pp. xx-yy, October, (2017). 
  
  %\bibitem{linux} ``The Linux Documentation Project'', \\\verb#http://www.tldp.org/LDP/intro-linux/html/chap_01.html# (2019).
  
\bibitem{Dongarra2017} John Dongarra et. al., ``With Extreme Computing, the Rules Have Changed'', \textit{Computing in Science Engineering}, \textbf{19}, pp. 52--62, (2017).
  
\bibitem{CUDA} John Nickolls, Ian Buck, Michael Garland and Kevin Skadron, ``Scalable Parallel Programming with CUDA'', \textit{Queue - GPU Computing}, \textbf{6}, pp. 40--53, (2008).
  
\bibitem{accelerators} David Tarditi, Sidd Puri, and Jose Oglesby, ``Accelerator: using data parallelism to program GPUs for general-purpose uses'',  \textit{SIGPLAN Not. 41}, \textbf{11}, pp. 325--335. DOI: https://doi.org/10.1145/1168918.1168898 (2006).

\bibitem{ECP} Anthony Scopatz and Kathryn Huff, \textit{Effective Computation in Physics: Field Guide to Research with Python}, O'Reilly Media, ISBN 978-1491901533, (2015).
  
\bibitem{slurm} SLURM.
  
  %\bibitem{windows7} Jorge Orchilles, \textit{Microsoft Windows 7 Administrator's Reference}, Syngress, Boston USA (2010).

\bibitem{ganglia} Matt Massie, Bernard Li, Brad Nicholes and Vladmir Vuksan, \textit{Monitoring with Ganglia}, O'Reilly Media, ISBN 978-1449329709, (2012).

  % Centos
\bibitem{centos} ``The CentOS Project'', \verb#https://www.centos.org# (2017).

\bibitem{yum} ``man7.org yum man page'', \verb#http://man7.org/linux/man-pages/man8/yum.8.html# (2017).
  
\bibitem{Boettiger} Carl Boettiger, ``An introduction to Docker for reproducible research'', \textit{SIGOPS Operating System Review}, \textbf{49}, pp. 71--79, (2015).

\bibitem{Tommaso} Paolo Di Tommaso et al. ``The Impact of Docker Containers on the Performance of Genomic Pipelines'' \textit{PeerJ}, \textbf{3}, (2015).

  % Virtualizacao (9 e 10)
\bibitem{vir1} A. J. Younge and G. C. Fox, ``Advanced Virtualization Techniques for High Performance Cloud Cyberinfrastructure'', \textit{2014 14th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing}, Chicago, IL, USA, pp. 583--586, May, (2014).
  
\bibitem{vir2} I. Sadooghi, J. H. Martin, T. Li, K. Brandstatter, K. Maheshwari, T. P. P. de Lacerda Ruivo, G. Garzoglio, S. Timm, Y. Zhao and I. Raicu, ``Understanding the Performance and Potential of Cloud Computing for Scientific Applications'', \textit{IEEE Transactions on Cloud Computing}, \textbf{5}, pp. 358--371, (2017).
  
\bibitem{linuxbook} Wirzenius and Lars, \textit{The  Linux System Administrator's Guide}, iUniverse incorporated (2000).

\bibitem{hal} Benjamin Depardon, Ga\"{e}l Le Mahec and Cyril S\'{e}guin, ``Analysis of Six Distributed File Systems'', Research Report, pp. 44, (2013).
  
\bibitem{gluster} Alex Davies and Alessandro Orsaria, ``Scale out with GlusterFS'', \textit{The Linux Journal}, \textbf{2013}, (2013).

%\bibitem{lustre} ``Lustre* Software Release 2.x: Operations Manual'', Editor: Intel Corporation, (2017).

\bibitem{paraview} Ayachit, Utkarsh, \textit{The ParaView Guide: A Parallel Visualization Application}, Kitware, ISBN 978-1930934306, (2015).

\bibitem{gpgpu}  John D. Owens, David Luebke, Naga Govindaraju, Mark Harris, Jens Kr\"{u}ger, Aaron E. Lefohn, and Tim Purcell, ``A Survey of General-Purpose Computation on Graphics Hardware'', \textit{Computer Graphics Forum}, \textbf{26(1)}, pp. 80--113, March (2007).

\bibitem{opencl} J. E. Stone, D. Gohara and G. Shi, ``OpenCL: A Parallel Programming Standard for Heterogeneous Computing Systems'', \textit{Computing in Science \& Engineering}, \textbf{12(3)}, pp. 66--73, May-June (2010).

  \bibitem{apache} APACHE.
    
\end{thebibliography}

% ---------------------------------------------------------
% Minha bibliografia usando arquivo externo
%\bibliographystyle{unsrt}
%\bibliography{bibli}


\end{document}
